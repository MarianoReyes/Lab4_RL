{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "1. ¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo?\n",
    "- Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.\n",
    "    \n",
    "La elección de la estrategia de exploración afecta la precisión de la evaluación de políticas en los métodos de Monte Carlo de las siguientes maneras:\n",
    "\n",
    "Exploring Starts: Esta estrategia asegura que cada estado inicial y acción posible tenga una probabilidad positiva de ser explorada. Esto significa que en la evaluación de políticas, se cubrirá todo el espacio de estados y acciones posibles, lo que puede llevar a una estimación más precisa del valor de la política en general. Sin embargo, esto puede ser computacionalmente costoso y puede llevar más tiempo para converger si el espacio de estados es grande.\n",
    "\n",
    "Soft Policy (Política Blanda): Las políticas blandas, como las estrategias epsilon-greedy, permiten cierta probabilidad de exploración, lo que implica que no siempre se tomará la acción de mayor valor esperado. Esto ayuda a explorar el espacio de estados y acciones, pero con un nivel de control sobre la cantidad de exploración. Cuanto mayor sea el nivel de exploración (por ejemplo, mayor epsilon), más se explorará, lo que puede llevar a una estimación más precisa pero menos eficiente del valor de la política. Menor exploración (menor epsilon) puede llevar a una estimación más rápida pero menos precisa.\n",
    "\n",
    "\n",
    "2. En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia.\n",
    "\n",
    "En el contexto del aprendizaje de Monte Carlo fuera de la póliza, la razón de muestreo de importancia es crucial para corregir el sesgo introducido por el hecho de que los datos no se recopilan según la política objetivo. Esta razón ajusta las estimaciones de valor para reflejar con precisión la política que se está evaluando.\n",
    "\n",
    "Convergencia y Estabilidad: Una alta razón de muestreo de importancia puede llevar a varianzas muy altas, lo que puede causar inestabilidad en las estimaciones de valor y ralentizar la convergencia. Sin embargo, asegura que las estimaciones sean no sesgadas. Para mejorar la estabilidad, se pueden utilizar técnicas como la normalización o el recorte de las razones de importancia.\n",
    "\n",
    "3. ¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de\n",
    "políticas deterministas.\n",
    "\n",
    "Políticas Epsilon-Greedy: Estas políticas permiten explorar aleatoriamente acciones que no son óptimas con una probabilidad epsilon, lo que facilita la exploración de diferentes trayectorias y la obtención de mejores políticas a largo plazo. Sin embargo, pueden ser menos eficientes en términos de convergencia y podrían aprender más lentamente debido a la exploración constante de acciones subóptimas.\n",
    "\n",
    "Políticas Deterministas: Estas políticas siempre seleccionan la acción de mayor valor esperado según el conocimiento actual, lo que puede llevar a una convergencia más rápida en escenarios donde la política óptima se encuentra rápidamente. Sin embargo, pueden quedar atrapadas en soluciones subóptimas si el espacio de estados no se explora adecuadamente.\n",
    "\n",
    "Las políticas blandas exploran más y pueden encontrar mejores soluciones a largo plazo, aunque más lentamente. Las políticas deterministas convergen más rápido pero pueden quedar atrapadas en soluciones subóptimas.\n",
    "\n",
    "4. ¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional. y velocidad de aprendizaje?\n",
    "\n",
    "Eficiencia de la Muestra:\n",
    "\n",
    "- Off-Policy: Permiten utilizar datos de políticas antiguas o diferentes, lo que puede ser útil para aprovechar datos históricos. Sin embargo, la eficiencia puede verse afectada debido a la necesidad de corregir el sesgo mediante la razón de muestreo de importancia.\n",
    "- On-Policy: Utilizan datos de la política actual, lo que asegura que las muestras sean directamente relevantes para la política que se está evaluando, pero requiere generar nuevos datos para cada política, lo que puede ser menos eficiente.\n",
    "\n",
    "Costo Computacional:\n",
    "\n",
    "- Off-Policy: Puede ser más costoso debido a la necesidad de calcular razones de muestreo de importancia y manejar varianzas altas.\n",
    "- On-Policy: Puede ser menos costoso en términos de computación, pero requiere datos frescos, lo que puede aumentar los costos de recolección de datos.\n",
    "\n",
    "Velocidad de Aprendizaje:\n",
    "\n",
    "- Off-Policy: Puede ser más lento debido a la alta varianza en las estimaciones, pero permite utilizar más datos potenciales.\n",
    "- On-Policy: Generalmente más rápido y directo, pero puede requerir más muestras para cubrir adecuadamente el espacio de estados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
